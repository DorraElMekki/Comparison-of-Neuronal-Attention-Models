{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 59647,
     "status": "ok",
     "timestamp": 1560157066995,
     "user": {
      "displayName": "sephiros sama",
      "photoUrl": "https://lh5.googleusercontent.com/-Aaghu78j1FA/AAAAAAAAAAI/AAAAAAAAImI/of29pyh0eh4/s64/photo.jpg",
      "userId": "04364851670955414673"
     },
     "user_tz": -120
    },
    "id": "OP9NqzeWnn7h",
    "outputId": "7d9c8b5a-ea31-4e99-9fb6-7a30d9f9cd7a"
   },
   "source": [
    "# Neuronal Attention model \n",
    "In this notebook, we test different configuration of Attention models \n",
    "\n",
    "## Technical details\n",
    "always use (Kernel -> Restart & Run All) to avoid mistakes in log files and others runtime errors.\n",
    "3 log files will be created for each run.\n",
    "\n",
    "## Visualisation using Tensor-board\n",
    "Enter this command \n",
    "\n",
    "tensorboard --logdir logs/100x100-4glimpse-12x12-4scales-128batch-100epochs\n",
    "\n",
    "or this one\n",
    "\n",
    "tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 59631,
     "status": "ok",
     "timestamp": 1560157066999,
     "user": {
      "displayName": "sephiros sama",
      "photoUrl": "https://lh5.googleusercontent.com/-Aaghu78j1FA/AAAAAAAAAAI/AAAAAAAAImI/of29pyh0eh4/s64/photo.jpg",
      "userId": "04364851670955414673"
     },
     "user_tz": -120
    },
    "id": "dY0TBzdLlHJh",
    "outputId": "12158ef8-f69d-4715-89eb-bd13efbb47a8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Python 3!\n",
      "/device:GPU:0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3878527118224350303\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4945621811\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10436485028097083923\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print('Using Python {}!'.format(sys.version_info[0]))\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.test.gpu_device_name() )\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "\n",
    "print(local_device_protos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63052,
     "status": "ok",
     "timestamp": 1560157070434,
     "user": {
      "displayName": "sephiros sama",
      "photoUrl": "https://lh5.googleusercontent.com/-Aaghu78j1FA/AAAAAAAAAAI/AAAAAAAAImI/of29pyh0eh4/s64/photo.jpg",
      "userId": "04364851670955414673"
     },
     "user_tz": -120
    },
    "id": "PrQr-uLiirca",
    "outputId": "104a775e-35ca-4de0-a148-dab1b6d39133"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Recurrent Models of Visual Attention V. Mnih et al.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "try:\n",
    "  xrange\n",
    "except NameError:\n",
    "  xrange=range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oadcenFJIHAt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4glimpse-12x12-4scales-128batch-100epochs-0.22std-RMSPropOptimizer0.0015'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfOptimizer = {\n",
    "    \"Adam\":            {\"method\": tf.train.AdamOptimizer, \"lr_min\":1e-4, \"lr_start\":1e-3 },\n",
    "    \"Adagrad\":         {\"method\": tf.train.AdagradOptimizer, \"lr_min\":0.2, \"lr_start\":0.3 },\n",
    "    \"Adadelta\":        {\"method\": tf.train.AdadeltaOptimizer, \"lr_min\":5.0 , \"lr_start\":12.0 },\n",
    "    \"RMSProp\":         {\"method\": tf.train.RMSPropOptimizer, \"lr_min\":0.0009 , \"lr_start\":0.0015 },\n",
    "    #\"ProximalAdagrad\": tf.train.ProximalAdagradOptimizer\n",
    "    #\"Ftrl\":            tf.train.FtrlOptimizer\n",
    "    #\"AdaMax\":          tf.train.AdaMaxOptimizer # require tensorflow 1.14 ; we use 1.13.1\n",
    "}\n",
    "class Config(object):\n",
    "\n",
    "  win_size = 12        ## Size of a scale (Sensor Bandwidth in pixels)     # Default: 12\n",
    "  num_glimpses = 4     ## Number of gimpses per image                      # Default: 4\n",
    "  num_scales = 4       ## Number of scales per glimpse                     # Default: 4\n",
    "\n",
    "  batch_size = 128     ## Size of the mini batch                           # Default: 128\n",
    "  eval_batch_size = batch_size #128 ## execute an evaluation of the performance of the model every n image\n",
    "\n",
    "  step = 100           ## Number of epochs                                   # Default: 100      # Best: 60\n",
    "    \n",
    "  optimizer = \"RMSProp\"  ## Optimizer                                      # Default: \"Adam\"\n",
    "\n",
    "  lr_start = tfOptimizer[optimizer][\"lr_start\"] #1e-3    ## Learning rate                                      # Default: Adam 1e-3\n",
    "  lr_min = tfOptimizer[optimizer][\"lr_min\"] #1e-4\n",
    "  decay = 0.97       ##                                                    # Default: 0.97\n",
    "  \n",
    "  #less important config  \n",
    "  loc_std = 0.22       ## Randomness in sampling the next location         # Default: 0.22     # Best: 0.5\n",
    "  original_size = 100  ## input image's size in pixels\n",
    "  num_channels = 1     ## Since we are dealing with gray scaled images, the input have only one channel\n",
    "                        # (!) the implmentation is not really taking this param. into consideration, do not change it\n",
    "  bandwidth = win_size**2\n",
    "  sensor_size = win_size**2 * num_channels * num_scales\n",
    "  minRadius = 8\n",
    "  hg_size = hl_size = 128\n",
    "  g_size = 256\n",
    "  cell_output_size = 256\n",
    "  loc_dim = 2\n",
    "  cell_size = 256\n",
    "  cell_out_size = cell_size\n",
    "  num_classes = 10\n",
    "  max_grad_norm = 5.\n",
    "\n",
    "  # Monte Carlo sampling\n",
    "  M = 10\n",
    "\n",
    "  # Run name\n",
    "  run_name = \"{}glimpse-{}x{}-{}scales-{}batch-{}epochs-{}std-{}Optimizer{}\".format(\n",
    "                                                     num_glimpses,\n",
    "                                                     win_size,\n",
    "                                                     win_size,\n",
    "                                                     num_scales,\n",
    "                                                     batch_size,\n",
    "                                                     step,\n",
    "                                                     loc_std,\n",
    "                                                     optimizer,\n",
    "                                                     lr_start\n",
    "                                                    )\n",
    "config = Config()\n",
    "config_text = str([(k,v) for k,v in Config.__dict__.items()])\n",
    "#config.run_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63044,
     "status": "ok",
     "timestamp": 1560157070437,
     "user": {
      "displayName": "sephiros sama",
      "photoUrl": "https://lh5.googleusercontent.com/-Aaghu78j1FA/AAAAAAAAAAI/AAAAAAAAImI/of29pyh0eh4/s64/photo.jpg",
      "userId": "04364851670955414673"
     },
     "user_tz": -120
    },
    "id": "d7hmXcp8nn7u",
    "outputId": "87752855-1c25-4d9e-e0f6-881d6fce6c46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created:\n",
      "run-4glimpse-12x12-4scales-128batch-100epochs-0.22std-RMSPropOptimizer0.0015.log\n",
      "./logs/4glimpse-12x12-4scales-128batch-100epochs-0.22std-RMSPropOptimizer0.0015\n",
      "model-4glimpse-12x12-4scales-128batch-100epochs-0.22std-RMSPropOptimizer0.0015.ckpt\n"
     ]
    }
   ],
   "source": [
    "basicConfigFileName = 'run-{}.log'.format(config.run_name)\n",
    "tfLogFile = \"./logs/\"+config.run_name\n",
    "savedModel_path =  \"model-{}.ckpt\".format(config.run_name)\n",
    "print(\"File created:\")\n",
    "print(basicConfigFileName)\n",
    "print(tfLogFile)\n",
    "print(savedModel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 77168,
     "status": "ok",
     "timestamp": 1560157084569,
     "user": {
      "displayName": "sephiros sama",
      "photoUrl": "https://lh5.googleusercontent.com/-Aaghu78j1FA/AAAAAAAAAAI/AAAAAAAAImI/of29pyh0eh4/s64/photo.jpg",
      "userId": "04364851670955414673"
     },
     "user_tz": -120
    },
    "id": "rXnNsyRKIGdq",
    "outputId": "deb4be11-e063-4d12-863f-cb6bdc1f5606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (54000, 10000)\n",
      "54000 train samples\n",
      "6000 validation samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(filename= basicConfigFileName,level=logging.DEBUG)\n",
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "rnn_cell = tf.nn.rnn_cell\n",
    "seq2seq = tf.contrib.legacy_seq2seq\n",
    "\n",
    "#mnist = input_data.read_data_sets('MNIST_data', one_hot=False)\n",
    "data = np.load('../data/mnist_digit_sample_8dsistortions9x9.npz')\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "x_train = np.reshape(data['X_train'], (-1, 10000))\n",
    "y_train = np.reshape(data['y_train'], (-1))\n",
    "x_va = np.reshape(data['X_valid'], (-1, 10000))\n",
    "y_va = np.reshape(data['y_valid'], (-1))\n",
    "x_test = np.reshape(data['X_test'], (-1, 10000))\n",
    "y_test = np.reshape(data['y_test'], (-1))\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_va.shape[0], 'validation samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NP7RbmZdmTYx"
   },
   "outputs": [],
   "source": [
    "input_shape = (config.original_size, config.original_size, 1)\n",
    "\n",
    "num_epochs = config.step\n",
    "\n",
    "loc_mean_arr = []\n",
    "sampled_loc_arr = []\n",
    "\n",
    "def get_next_input(output, i):\n",
    "  loc, loc_mean = loc_net(output)\n",
    "  gl_next = gl(loc)\n",
    "  loc_mean_arr.append(loc_mean)\n",
    "  sampled_loc_arr.append(loc)\n",
    "  return gl_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xVANDfcY6sNt"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "distributions = tf.contrib.distributions\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.0, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def loglikelihood(mean_arr, sampled_arr, sigma):\n",
    "  mu = tf.stack(mean_arr)  # mu = [timesteps, batch_sz, loc_dim]\n",
    "  sampled = tf.stack(sampled_arr)  # same shape as mu\n",
    "  gaussian = distributions.Normal(mu, sigma)\n",
    "  logll = gaussian.log_prob(sampled)  # [timesteps, batch_sz, loc_dim]\n",
    "  logll = tf.reduce_sum(logll, 2)\n",
    "  logll = tf.transpose(logll)  # [batch_sz, timesteps]\n",
    "  return logll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xyTEfjE63M6H"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class GlimpseNet(object):\n",
    "  \"\"\"Glimpse network.\n",
    "\n",
    "  Take glimpse location input and output features for RNN.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, config, images_ph):\n",
    "    self.original_size = config.original_size\n",
    "    self.num_channels = config.num_channels\n",
    "    self.sensor_size = config.sensor_size\n",
    "    self.win_size = config.win_size\n",
    "    self.minRadius = config.minRadius\n",
    "    self.num_scales = config.num_scales\n",
    "\n",
    "    self.hg_size = config.hg_size\n",
    "    self.hl_size = config.hl_size\n",
    "    self.g_size = config.g_size\n",
    "    self.loc_dim = config.loc_dim\n",
    "\n",
    "    self.images_ph = images_ph\n",
    "\n",
    "    self.init_weights()\n",
    "\n",
    "  def init_weights(self):\n",
    "    \"\"\" Initialize all the trainable weights.\"\"\"\n",
    "    self.w_g0 = weight_variable((self.sensor_size, self.hg_size))\n",
    "    self.b_g0 = bias_variable((self.hg_size,))\n",
    "    self.w_l0 = weight_variable((self.loc_dim, self.hl_size))\n",
    "    self.b_l0 = bias_variable((self.hl_size,))\n",
    "    self.w_g1 = weight_variable((self.hg_size, self.g_size))\n",
    "    self.b_g1 = bias_variable((self.g_size,))\n",
    "    self.w_l1 = weight_variable((self.hl_size, self.g_size))\n",
    "    self.b_l1 = weight_variable((self.g_size,))\n",
    "\n",
    "  def get_glimpse(self, loc):\n",
    "    \"\"\"Take glimpse on the original images.\n",
    "\n",
    "    :param loc: 2D tuple locations, values between [-1.0, 1.0]\n",
    "    :return: glimpse vector\n",
    "    \"\"\"\n",
    "    imgs = tf.reshape(self.images_ph, [\n",
    "        tf.shape(self.images_ph)[0], self.original_size, self.original_size,\n",
    "        self.num_channels\n",
    "    ])\n",
    "\n",
    "    glimpse_all_scales = []\n",
    "    for scale in range(1, self.num_scales + 1):\n",
    "      glimpse_imgs = tf.image.extract_glimpse(imgs,\n",
    "                                              [self.win_size * scale, self.win_size * scale], loc) # BHWC\n",
    "\n",
    "      glimpse_imgs = tf.image.resize_bilinear(glimpse_imgs, (self.win_size, self.win_size)) # BHWC\n",
    "      glimpse_imgs = tf.reshape(glimpse_imgs, [\n",
    "          tf.shape(loc)[0], self.win_size * self.win_size * self.num_channels\n",
    "      ]) #(B, H * W * C)\n",
    "\n",
    "      glimpse_all_scales.append(glimpse_imgs)\n",
    "\n",
    "    return tf.stack(glimpse_all_scales, axis=1) # (B, H * W * C * S)\n",
    "\n",
    "  def __call__(self, loc):\n",
    "    glimpse_input = self.get_glimpse(loc) # (B, H * W * C * S)\n",
    "    glimpse_input = tf.reshape(glimpse_input,\n",
    "                               (tf.shape(loc)[0], self.sensor_size))\n",
    "    g = tf.nn.relu(tf.nn.xw_plus_b(glimpse_input, self.w_g0, self.b_g0))\n",
    "    g = tf.nn.xw_plus_b(g, self.w_g1, self.b_g1)\n",
    "    l = tf.nn.relu(tf.nn.xw_plus_b(loc, self.w_l0, self.b_l0))\n",
    "    l = tf.nn.xw_plus_b(l, self.w_l1, self.b_l1)\n",
    "    g = tf.nn.relu(g + l)\n",
    "    return g\n",
    "\n",
    "\n",
    "class LocNet(object):\n",
    "  \"\"\"Location network.\n",
    "\n",
    "  Take output from other network and produce and sample the next location.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, config):\n",
    "    self.loc_dim = config.loc_dim\n",
    "    self.input_dim = config.cell_output_size\n",
    "    self.loc_std = config.loc_std\n",
    "    self._sampling = True  # if True, the next location will be biaised using a random distribution\n",
    "\n",
    "    self.init_weights()\n",
    "\n",
    "  def init_weights(self):\n",
    "    self.w = weight_variable((self.input_dim, self.loc_dim))\n",
    "    self.b = bias_variable((self.loc_dim,))\n",
    "\n",
    "  def __call__(self, input):\n",
    "    mean = tf.clip_by_value(tf.nn.xw_plus_b(input, self.w, self.b), -1., 1.)\n",
    "    mean = tf.stop_gradient(mean)\n",
    "    if self._sampling:\n",
    "      loc = mean + tf.random_normal(\n",
    "          (tf.shape(input)[0], self.loc_dim), stddev=self.loc_std)\n",
    "      loc = tf.clip_by_value(loc, -1., 1.)\n",
    "    else:\n",
    "      loc = mean\n",
    "    loc = tf.stop_gradient(loc)\n",
    "    return loc, mean\n",
    "\n",
    "  @property\n",
    "  def sampling(self):\n",
    "    return self._sampling\n",
    "\n",
    "  @sampling.setter\n",
    "  def sampling(self, sampling):\n",
    "    self._sampling = sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## glimpse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79259,
     "status": "ok",
     "timestamp": 1560157086673,
     "user": {
      "displayName": "sephiros sama",
      "photoUrl": "https://lh5.googleusercontent.com/-Aaghu78j1FA/AAAAAAAAAAI/AAAAAAAAImI/of29pyh0eh4/s64/photo.jpg",
      "userId": "04364851670955414673"
     },
     "user_tz": -120
    },
    "id": "y6qr4Ghmm57I",
    "outputId": "9f4b6b01-887a-4b8c-83d1-1039a387a3b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tensor-env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-9-b4e320f247c3>:25: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "#import glimpse.py\n",
    "# placeholders\n",
    "images_ph = tf.placeholder(tf.float32,\n",
    "                           [None, config.original_size * config.original_size *\n",
    "                            config.num_channels])\n",
    "labels_ph = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "# Monte Carlo sampling, duplicate M times, see Eqn (2)\n",
    "images_expanded = tf.tile(images_ph, [config.M, 1])\n",
    "labels_expanded = tf.tile(labels_ph, [config.M])\n",
    "\n",
    "# Build the aux nets.\n",
    "with tf.variable_scope('glimpse_net'):\n",
    "  # gl = GlimpseNet(config, images_ph)\n",
    "  gl = GlimpseNet(config, images_expanded)\n",
    "with tf.variable_scope('loc_net'):\n",
    "  loc_net = LocNet(config)\n",
    "\n",
    "# number of examples\n",
    "# N = tf.shape(images_ph)[0]\n",
    "N = tf.shape(images_expanded)[0]\n",
    "init_loc = tf.random_uniform((N, 2), minval=-1, maxval=1)\n",
    "init_glimpse = gl(init_loc)\n",
    "# Core network.\n",
    "lstm_cell = rnn_cell.LSTMCell(config.cell_size, state_is_tuple=True)\n",
    "init_state = lstm_cell.zero_state(N, tf.float32)\n",
    "inputs = [init_glimpse]\n",
    "inputs.extend([0] * (config.num_glimpses))\n",
    "outputs, _ = seq2seq.rnn_decoder(\n",
    "    inputs, init_state, lstm_cell, loop_function=get_next_input)\n",
    "\n",
    "# Time independent baselines\n",
    "with tf.variable_scope('baseline'):\n",
    "  w_baseline = weight_variable((config.cell_output_size, 1))\n",
    "  b_baseline = bias_variable((1,))\n",
    "baselines = []\n",
    "for t, output in enumerate(outputs[1:]):\n",
    "  baseline_t = tf.nn.xw_plus_b(output, w_baseline, b_baseline)\n",
    "  baseline_t = tf.squeeze(baseline_t)\n",
    "  baselines.append(baseline_t)\n",
    "baselines = tf.stack(baselines)  # [timesteps, batch_sz]\n",
    "baselines = tf.transpose(baselines)  # [batch_sz, timesteps]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the last step only.\n",
    "output = outputs[-1]\n",
    "# Build classification network.\n",
    "with tf.variable_scope('cls'):\n",
    "  w_logit = weight_variable((config.cell_output_size, config.num_classes))\n",
    "  b_logit = bias_variable((config.num_classes,))\n",
    "logits = tf.nn.xw_plus_b(output, w_logit, b_logit)\n",
    "softmax = tf.nn.softmax(logits)\n",
    "correct_prediction = tf.equal(tf.argmax(softmax,1), labels_expanded)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# average statistics after Monte Carlo sampling \"M\"\n",
    "avg_softmax = tf.reshape(softmax, [config.M, -1, config.num_classes])\n",
    "avg_softmax = tf.reduce_mean(avg_softmax, axis=0) # (B, num_classes)\n",
    "avg_y_pred = tf.argmax(avg_softmax, axis=1) #(B, )\n",
    "avg_acc = tf.reduce_mean(tf.cast(tf.equal(avg_y_pred, labels_ph), tf.float32))\n",
    "\n",
    "# cross-entropy.\n",
    "xent = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels_expanded)\n",
    "xent = tf.reduce_mean(xent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-ff39f6e7cf00>:23: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tensor-env\\lib\\site-packages\\tensorflow\\python\\ops\\distributions\\normal.py:160: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tensor-env\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# 0/1 reward.\n",
    "y_pred = tf.argmax(logits, 1)\n",
    "reward = tf.cast(tf.equal(y_pred, labels_expanded), tf.float32)\n",
    "rewards = tf.expand_dims(reward, 1)  # [batch_sz, 1]\n",
    "rewards = tf.tile(rewards, (1, config.num_glimpses))  # [batch_sz, timesteps]\n",
    "logll = loglikelihood(loc_mean_arr, sampled_loc_arr, config.loc_std)\n",
    "advs = rewards - tf.stop_gradient(baselines)\n",
    "logllratio = tf.reduce_mean(logll * advs)\n",
    "reward = tf.reduce_mean(reward)\n",
    "\n",
    "baselines_mse = tf.reduce_mean(tf.square((rewards - baselines)))\n",
    "var_list = tf.trainable_variables()\n",
    "\n",
    "# hybrid loss\n",
    "loss = -logllratio + xent + baselines_mse  # `-` for minimize\n",
    "grads = tf.gradients(loss, var_list)\n",
    "grads, _ = tf.clip_by_global_norm(grads, config.max_grad_norm)\n",
    "\n",
    "# learning rate\n",
    "global_step = tf.get_variable(\n",
    "    'global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n",
    "training_steps_per_epoch = x_train.shape[0] // config.batch_size\n",
    "starter_learning_rate = config.lr_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decay per training epoch\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    starter_learning_rate,\n",
    "    global_step,\n",
    "    training_steps_per_epoch,\n",
    "    config.decay,\n",
    "    staircase=True)\n",
    "learning_rate = tf.maximum(learning_rate, config.lr_min)\n",
    "\n",
    "opt = tfOptimizer[config.optimizer][\"method\"](learning_rate)\n",
    "train_op = opt.apply_gradients(zip(grads, var_list), global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard logging\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "tf.summary.scalar(\"reward\", reward)\n",
    "tf.summary.scalar(\"xent\", xent)\n",
    "tf.summary.scalar(\"baselines_mse\", baselines_mse)\n",
    "tf.summary.scalar(\"logllratio\", logllratio)\n",
    "tf.summary.scalar(\"avg_accuracy\", avg_acc)\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sTgt2N39nn7_"
   },
   "outputs": [],
   "source": [
    "# stats\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 217772,
     "status": "ok",
     "timestamp": 1560157225194,
     "user": {
      "displayName": "sephiros sama",
      "photoUrl": "https://lh5.googleusercontent.com/-Aaghu78j1FA/AAAAAAAAAAI/AAAAAAAAImI/of29pyh0eh4/s64/photo.jpg",
      "userId": "04364851670955414673"
     },
     "user_tz": -120
    },
    "id": "phcl3M-0KE0z",
    "outputId": "3000573c-6820-4ab6-d801-a240dcc28432"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tensor-env\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "num_epochs:  0\n",
      "time_per_epoch:  47.41077756881714\n",
      "time left:  1:19:01.077757\n",
      "num_epochs:  1\n",
      "time_per_epoch:  46.520928621292114\n",
      "time left:  1:17:29.667301\n",
      "num_epochs:  2\n",
      "time_per_epoch:  49.122639417648315\n",
      "time left:  1:17:53.140197\n",
      "num_epochs:  3\n",
      "time_per_epoch:  48.107606649398804\n",
      "time left:  1:17:15.724983\n",
      "num_epochs:  4\n",
      "time_per_epoch:  48.13325834274292\n",
      "time left:  1:16:34.505763\n",
      "num_epochs:  5\n",
      "time_per_epoch:  49.63938546180725\n",
      "time left:  1:16:14.861390\n",
      "num_epochs:  6\n",
      "time_per_epoch:  50.08082938194275\n",
      "time left:  1:15:52.560180\n",
      "num_epochs:  7\n",
      "time_per_epoch:  50.12198185920715\n",
      "time left:  1:15:23.803796\n",
      "num_epochs:  8\n",
      "time_per_epoch:  49.34840440750122\n",
      "time left:  1:14:42.381200\n",
      "num_epochs:  9\n",
      "time_per_epoch:  51.58302903175354\n",
      "time left:  1:14:19.708644\n",
      "num_epochs:  10\n",
      "time_per_epoch:  50.9014630317688\n",
      "time left:  1:13:46.194567\n",
      "num_epochs:  11\n",
      "time_per_epoch:  52.10748505592346\n",
      "time left:  1:13:18.727256\n",
      "num_epochs:  12\n",
      "time_per_epoch:  53.262083530426025\n",
      "time left:  1:12:55.291629\n",
      "num_epochs:  13\n",
      "time_per_epoch:  50.824047327041626\n",
      "time left:  1:12:12.444484\n",
      "num_epochs:  14\n",
      "time_per_epoch:  51.3367223739624\n",
      "time left:  1:11:31.472592\n",
      "num_epochs:  15\n",
      "time_per_epoch:  53.022451877593994\n",
      "time left:  1:10:58.155290\n",
      "num_epochs:  16\n",
      "time_per_epoch:  49.89028549194336\n",
      "time left:  1:10:07.043147\n",
      "num_epochs:  17\n",
      "time_per_epoch:  49.57095956802368\n",
      "time left:  1:09:14.594319\n",
      "num_epochs:  18\n",
      "time_per_epoch:  51.1849946975708\n",
      "time left:  1:08:29.418560\n",
      "num_epochs:  19\n",
      "time_per_epoch:  50.777387380599976\n",
      "time left:  1:07:41.990881\n",
      "num_epochs:  20\n",
      "time_per_epoch:  49.616934299468994\n",
      "time left:  1:06:49.827337\n",
      "num_epochs:  21\n",
      "time_per_epoch:  49.49999213218689\n",
      "time left:  1:05:57.467899\n",
      "num_epochs:  22\n",
      "time_per_epoch:  51.57362413406372\n",
      "time left:  1:05:12.392815\n",
      "num_epochs:  23\n",
      "time_per_epoch:  51.37381625175476\n",
      "time left:  1:04:26.131846\n",
      "num_epochs:  24\n",
      "time_per_epoch:  49.71730399131775\n",
      "time left:  1:03:34.429085\n",
      "num_epochs:  25\n",
      "time_per_epoch:  52.88231158256531\n",
      "time left:  1:02:52.008807\n",
      "num_epochs:  26\n",
      "time_per_epoch:  50.21802282333374\n",
      "time left:  1:02:01.508628\n",
      "num_epochs:  27\n",
      "time_per_epoch:  49.46182608604431\n",
      "time left:  1:01:09.057090\n",
      "num_epochs:  28\n",
      "time_per_epoch:  51.827171325683594\n",
      "time left:  1:00:22.684320\n",
      "num_epochs:  29\n",
      "time_per_epoch:  50.43326449394226\n",
      "time left:  0:59:32.651370\n",
      "num_epochs:  30\n",
      "time_per_epoch:  51.17368006706238\n",
      "time left:  0:58:44.262184\n",
      "num_epochs:  31\n",
      "time_per_epoch:  50.191664695739746\n",
      "time left:  0:57:53.583717\n",
      "num_epochs:  32\n",
      "time_per_epoch:  49.47149896621704\n",
      "time left:  0:57:01.448591\n",
      "num_epochs:  33\n",
      "time_per_epoch:  49.39113187789917\n",
      "time left:  0:56:09.311778\n",
      "num_epochs:  34\n",
      "time_per_epoch:  49.667508125305176\n",
      "time left:  0:55:17.853028\n",
      "num_epochs:  35\n",
      "time_per_epoch:  49.53204846382141\n",
      "time left:  0:54:26.252873\n",
      "num_epochs:  36\n",
      "time_per_epoch:  50.15570592880249\n",
      "time left:  0:53:35.841318\n",
      "num_epochs:  37\n",
      "time_per_epoch:  50.332266330718994\n",
      "time left:  0:52:45.735950\n",
      "num_epochs:  38\n",
      "time_per_epoch:  49.28455376625061\n",
      "time left:  0:51:53.954872\n",
      "num_epochs:  39\n",
      "time_per_epoch:  48.81668949127197\n",
      "time left:  0:51:01.582000\n",
      "num_epochs:  40\n",
      "time_per_epoch:  49.06184411048889\n",
      "time left:  0:50:09.742826\n",
      "num_epochs:  41\n",
      "time_per_epoch:  48.93611812591553\n",
      "time left:  0:49:17.857839\n",
      "num_epochs:  42\n",
      "time_per_epoch:  50.01018977165222\n",
      "time left:  0:48:27.558756\n",
      "num_epochs:  43\n",
      "time_per_epoch:  49.61299967765808\n",
      "time left:  0:47:36.759555\n",
      "num_epochs:  44\n",
      "time_per_epoch:  49.60900592803955\n",
      "time left:  0:46:46.008067\n",
      "num_epochs:  45\n",
      "time_per_epoch:  49.765032052993774\n",
      "time left:  0:45:55.492754\n",
      "num_epochs:  46\n",
      "time_per_epoch:  51.00227928161621\n",
      "time left:  0:45:06.430831\n",
      "num_epochs:  47\n",
      "time_per_epoch:  49.66028714179993\n",
      "time left:  0:44:15.805147\n",
      "num_epochs:  48\n",
      "time_per_epoch:  50.655561447143555\n",
      "time left:  0:43:26.276158\n",
      "num_epochs:  49\n",
      "time_per_epoch:  51.418078899383545\n",
      "time left:  0:42:37.479788\n",
      "num_epochs:  50\n",
      "time_per_epoch:  51.2218873500824\n",
      "time left:  0:41:48.388279\n",
      "num_epochs:  51\n",
      "time_per_epoch:  50.73172044754028\n",
      "time left:  0:40:58.752874\n",
      "num_epochs:  52\n",
      "time_per_epoch:  51.1420578956604\n",
      "time left:  0:40:09.447657\n",
      "num_epochs:  53\n",
      "time_per_epoch:  51.478986501693726\n",
      "time left:  0:39:20.366785\n",
      "num_epochs:  54\n",
      "time_per_epoch:  52.19135665893555\n",
      "time left:  0:38:31.795342\n",
      "num_epochs:  55\n",
      "time_per_epoch:  54.48721432685852\n",
      "time left:  0:37:44.939468\n",
      "num_epochs:  56\n",
      "time_per_epoch:  52.655667304992676\n",
      "time left:  0:36:56.401197\n",
      "num_epochs:  57\n",
      "time_per_epoch:  51.52412486076355\n",
      "time left:  0:36:06.882045\n",
      "num_epochs:  58\n",
      "time_per_epoch:  51.10028004646301\n",
      "time left:  0:35:16.993207\n",
      "num_epochs:  59\n",
      "time_per_epoch:  50.276291370391846\n",
      "time left:  0:34:26.500930\n",
      "num_epochs:  60\n",
      "time_per_epoch:  50.08817386627197\n",
      "time left:  0:33:35.892377\n",
      "num_epochs:  61\n",
      "time_per_epoch:  49.86725473403931\n",
      "time left:  0:32:45.162274\n",
      "num_epochs:  62\n",
      "time_per_epoch:  53.89661622047424\n",
      "time left:  0:31:56.889338\n",
      "num_epochs:  63\n",
      "time_per_epoch:  53.463963747024536\n",
      "time left:  0:31:08.190535\n",
      "num_epochs:  64\n",
      "time_per_epoch:  52.54260754585266\n",
      "time left:  0:30:18.834821\n",
      "num_epochs:  65\n",
      "time_per_epoch:  52.43319869041443\n",
      "time left:  0:29:29.325044\n",
      "num_epochs:  66\n",
      "time_per_epoch:  53.27354407310486\n",
      "time left:  0:28:40.153908\n",
      "num_epochs:  67\n",
      "time_per_epoch:  52.120084047317505\n",
      "time left:  0:27:50.302347\n",
      "num_epochs:  68\n",
      "time_per_epoch:  50.65370583534241\n",
      "time left:  0:26:59.704971\n",
      "num_epochs:  69\n",
      "time_per_epoch:  51.02506709098816\n",
      "time left:  0:26:09.270447\n",
      "num_epochs:  70\n",
      "time_per_epoch:  50.97377824783325\n",
      "time left:  0:25:18.797615\n",
      "num_epochs:  71\n",
      "time_per_epoch:  51.381035804748535\n",
      "time left:  0:24:28.475319\n",
      "num_epochs:  72\n",
      "time_per_epoch:  50.6671040058136\n",
      "time left:  0:23:37.850136\n",
      "num_epochs:  73\n",
      "time_per_epoch:  51.047303676605225\n",
      "time left:  0:22:47.362153\n",
      "num_epochs:  74\n",
      "time_per_epoch:  50.33273673057556\n",
      "time left:  0:21:56.611883\n",
      "num_epochs:  75\n",
      "time_per_epoch:  50.33796572685242\n",
      "time left:  0:21:05.873967\n",
      "num_epochs:  76\n",
      "time_per_epoch:  50.55533003807068\n",
      "time left:  0:20:15.214189\n",
      "num_epochs:  77\n",
      "time_per_epoch:  50.36117744445801\n",
      "time left:  0:19:24.499839\n",
      "num_epochs:  78\n",
      "time_per_epoch:  50.86349558830261\n",
      "time left:  0:18:33.934596\n",
      "num_epochs:  79\n",
      "time_per_epoch:  50.67953538894653\n",
      "time left:  0:17:43.313583\n",
      "num_epochs:  80\n",
      "time_per_epoch:  50.25527215003967\n",
      "time left:  0:16:52.586094\n",
      "num_epochs:  81\n",
      "time_per_epoch:  50.27421689033508\n",
      "time left:  0:16:01.874745\n",
      "num_epochs:  82\n",
      "time_per_epoch:  52.88773703575134\n",
      "time left:  0:15:11.740692\n",
      "num_epochs:  83\n",
      "time_per_epoch:  51.24595785140991\n",
      "time left:  0:14:21.208995\n",
      "num_epochs:  84\n",
      "time_per_epoch:  50.94209122657776\n",
      "time left:  0:13:30.603060\n",
      "num_epochs:  85\n",
      "time_per_epoch:  50.24150896072388\n",
      "time left:  0:12:39.867089\n",
      "num_epochs:  86\n",
      "time_per_epoch:  50.61613416671753\n",
      "time left:  0:11:49.202738\n",
      "num_epochs:  87\n",
      "time_per_epoch:  50.137967586517334\n",
      "time left:  0:10:58.468674\n",
      "num_epochs:  88\n",
      "time_per_epoch:  51.120596170425415\n",
      "time left:  0:10:07.880495\n",
      "num_epochs:  89\n",
      "time_per_epoch:  50.419159173965454\n",
      "time left:  0:09:17.194875\n",
      "num_epochs:  90\n",
      "time_per_epoch:  50.08469843864441\n",
      "time left:  0:08:26.478450\n",
      "num_epochs:  91\n",
      "time_per_epoch:  50.42662310600281\n",
      "time left:  0:07:35.808964\n",
      "num_epochs:  92\n",
      "time_per_epoch:  51.246949195861816\n",
      "time left:  0:06:45.215266\n",
      "num_epochs:  93\n",
      "time_per_epoch:  50.44645810127258\n",
      "time left:  0:05:54.548058\n",
      "num_epochs:  94\n",
      "time_per_epoch:  51.49342203140259\n",
      "time left:  0:05:03.951622\n",
      "num_epochs:  95\n",
      "time_per_epoch:  50.861292362213135\n",
      "time left:  0:04:13.303575\n",
      "num_epochs:  96\n",
      "time_per_epoch:  51.483702182769775\n",
      "time left:  0:03:22.676839\n",
      "num_epochs:  97\n",
      "time_per_epoch:  50.11476993560791\n",
      "time left:  0:02:31.990688\n",
      "num_epochs:  98\n",
      "time_per_epoch:  50.555567026138306\n",
      "time left:  0:01:41.324983\n",
      "num_epochs:  99\n",
      "time_per_epoch:  51.22229313850403\n",
      "time left:  0:00:50.668090\n",
      "Model saved in file: model-4glimpse-12x12-4scales-128batch-100epochs-0.22std-RMSPropOptimizer0.0015.ckpt\n",
      "time:  5076.11937379837\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.initialize_all_variables())\n",
    "  writer = tf.summary.FileWriter(logdir=tfLogFile, graph=tf.get_default_graph())\n",
    "  start___time = time.time()\n",
    "  for epoch in xrange(num_epochs):\n",
    "    print(\"num_epochs: \",epoch)\n",
    "    start_epoch = time.time()\n",
    "  \n",
    "    num_batches = x_train.shape[0] // config.batch_size\n",
    "    num_samples = num_batches * config.batch_size\n",
    "    avg_loss = 0.\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "      start = batch * config.batch_size\n",
    "      end = (batch + 1) * config.batch_size\n",
    "      images, labels = x_train[start:end], y_train[start:end]\n",
    "\n",
    "      loc_net.samping = True\n",
    "      avg_acc_val, softmax_val, adv_val, baselines_val, rewards_val, baselines_mse_val, xent_val, logllratio_val, \\\n",
    "          reward_val, loss_val, lr_val, _, summary_val = sess.run(\n",
    "              [avg_acc, softmax, advs, baselines, rewards, baselines_mse, xent, logllratio,\n",
    "               reward, loss, learning_rate, train_op, summary_op],\n",
    "              feed_dict={\n",
    "                  images_ph: images,\n",
    "                  labels_ph: labels\n",
    "              })\n",
    "      writer.add_summary(summary_val, epoch * num_batches + batch)\n",
    "\n",
    "      avg_loss += loss_val / num_batches\n",
    "\n",
    "      if batch and batch % 100 == 0:\n",
    "        logging.info('epoch {}: batch: {}/{}'.format(epoch, batch, num_batches - 1))\n",
    "        logging.info('epoch {}: avg_accuracy: {}'.format(epoch, avg_acc_val))\n",
    "        logging.info('epoch {}: lr = {:3.6f}'.format(epoch, lr_val))\n",
    "        logging.info(\n",
    "            'epoch {}: reward = {:3.4f}\\tloss = {:3.4f}\\txent = {:3.4f}'.format(\n",
    "                epoch, reward_val, loss_val, xent_val))\n",
    "        logging.info('llratio = {:3.4f}\\tbaselines_mse = {:3.4f}'.format(\n",
    "            logllratio_val, baselines_mse_val))\n",
    "        logging.debug('baselines = {}\\trewards = {}'.format(baselines_val, rewards_val))\n",
    "\n",
    "    # if epoch and epoch % training_steps_per_epoch == 0:\n",
    "    if True: # print each epoch\n",
    "      # Evaluation\n",
    "      for dataset in [(x_va, y_va,'va')]:\n",
    "        num_batches = dataset[0].shape[0] // config.eval_batch_size\n",
    "        correct_cnt = 0\n",
    "        num_samples = num_batches * config.eval_batch_size\n",
    "        loc_net.sampling = True\n",
    "        for test_step in xrange(num_batches):\n",
    "          images, labels = dataset[0][test_step * config.eval_batch_size : (test_step+1) * config.eval_batch_size], dataset[1][test_step * config.eval_batch_size : (test_step+1) * config.eval_batch_size]\n",
    "\n",
    "          avg_y_pred_val = sess.run(avg_y_pred,\n",
    "                                 feed_dict={\n",
    "                                     images_ph: images,\n",
    "                                     labels_ph: labels\n",
    "                                 })\n",
    "\n",
    "          correct_cnt += np.sum(avg_y_pred_val == labels)\n",
    "        acc = correct_cnt / num_samples\n",
    "\n",
    "        logging.info('epoch {}: valid_accuracy = {}'.format(epoch, acc))\n",
    "    \n",
    "    print(\"time_per_epoch: \",str(time.time() - start_epoch) )\n",
    "    logging.info('time_per_epoch: {}'.format(time.time() - start_epoch))\n",
    "    print(\"time left: \",str(datetime.timedelta(seconds=(time.time() - start___time)/(1+epoch) * (num_epochs-epoch) )))\n",
    "  logging.info('Training_time = {}'.format(time.time() - start___time))\n",
    "  for dataset in [(x_test, y_test, 'test')]:\n",
    "    num_batches = dataset[0].shape[0] // config.eval_batch_size\n",
    "    correct_cnt = 0\n",
    "    num_samples = num_batches * config.eval_batch_size\n",
    "    loc_net.sampling = True\n",
    "    for test_step in xrange(num_batches):\n",
    "      images, labels = dataset[0][test_step * config.eval_batch_size: (test_step + 1) * config.eval_batch_size], \\\n",
    "                       dataset[1][test_step * config.eval_batch_size: (test_step + 1) * config.eval_batch_size]\n",
    "\n",
    "      avg_y_pred_val = sess.run(avg_y_pred,\n",
    "                                feed_dict={\n",
    "                                  images_ph: images,\n",
    "                                  labels_ph: labels\n",
    "                                })\n",
    "\n",
    "      correct_cnt += np.sum(avg_y_pred_val == labels)\n",
    "    acc = correct_cnt / num_samples\n",
    "    logging.info('test_accuracy = {}'.format(acc))\n",
    "    \n",
    "  save_path = saver.save(sess, savedModel_path)\n",
    "  logging.info('Model saved in file: {}'.format(save_path))\n",
    "  print('Model saved in file: {}'.format(save_path))\n",
    "  logging.info('total time = {}'.format(time.time() - start___time))\n",
    "  print(\"time: \",time.time() - start___time)\n",
    "  \n",
    "    \n",
    "  meta = tf.SummaryMetadata()\n",
    "  meta.plugin_data.plugin_name = \"text\"\n",
    "  summary = tf.Summary()\n",
    "\n",
    "  gpu_text = str(str(local_device_protos))\n",
    "  gpu_tensor = tf.make_tensor_proto(gpu_text, dtype=tf.string)\n",
    "  summary.value.add(tag=\"gpu_tag\", metadata=meta, tensor=gpu_tensor)\n",
    "    \n",
    "  config_tensor = tf.make_tensor_proto(config_text, dtype=tf.string)\n",
    "  summary.value.add(tag=\"config_tag\", metadata=meta, tensor=config_tensor)\n",
    "    \n",
    "  writer.add_summary(summary)\n",
    "  logging.info('gpu = {}'.format(str(local_device_protos)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: model-4glimpse-12x12-4scales-128batch-100epochs-0.22std-RMSPropOptimizer0.0015.ckpt\n"
     ]
    }
   ],
   "source": [
    "print('Model saved in file: {}'.format(save_path))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ram.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:tensor-env] *",
   "language": "python",
   "name": "conda-env-tensor-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
